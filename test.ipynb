{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import supervision as sv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = YOLO(\"yolov8s.pt\")\n",
    "classifier = YOLO(\"models/pose_classifier.pt\")\n",
    "byte_tracker = sv.ByteTrack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"fall\", \"standing\", \"sitting\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FALL=30\n",
    "MAX_NOT_MOVING = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Projects\\PeopleFallDetection\\data\\stand.jpg: 224x224 no fall 0.67, sitting 0.30, fall 0.03, 20.9ms\n",
      "Speed: 1.7ms preprocess, 20.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "res = classifier(\"data/stand.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0294, 0.6745, 0.2962])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].probs.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_offsets(bbox1, bbox2):\n",
    "    '''\n",
    "    compute differences between previous and current bboxes\n",
    "    '''\n",
    "    dx = (bbox2[0]-bbox1[0])**2 + (bbox2[2]-bbox1[2])**2\n",
    "    dy = (bbox2[1]-bbox1[1])**2 + (bbox2[3]-bbox1[3])**2\n",
    "    \n",
    "    return [dx, dy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n"
     ]
    }
   ],
   "source": [
    "if compute_offsets([2,3, 5,6], [2,3, 5,6])== [0, 0]:\n",
    "    print(\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 334.1ms\n",
      "Speed: 0.0ms preprocess, 334.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.63, no fall 0.32, sitting 0.05, 9.3ms\n",
      "Speed: 0.0ms preprocess, 9.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.66, no fall 0.29, sitting 0.05, 15.3ms\n",
      "Speed: 1.0ms preprocess, 15.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.69, no fall 0.27, sitting 0.04, 17.7ms\n",
      "Speed: 0.0ms preprocess, 17.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.62, no fall 0.33, sitting 0.04, 3.7ms\n",
      "Speed: 1.2ms preprocess, 3.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.73, no fall 0.24, sitting 0.03, 13.4ms\n",
      "Speed: 1.0ms preprocess, 13.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 357.0ms\n",
      "Speed: 0.0ms preprocess, 357.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.47, no fall 0.47, sitting 0.06, 7.5ms\n",
      "Speed: 0.0ms preprocess, 7.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.55, no fall 0.39, sitting 0.06, 11.7ms\n",
      "Speed: 9.0ms preprocess, 11.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.48, no fall 0.47, sitting 0.05, 12.5ms\n",
      "Speed: 0.0ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.48, no fall 0.46, sitting 0.06, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.62, no fall 0.33, sitting 0.05, 13.4ms\n",
      "Speed: 0.0ms preprocess, 13.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 318.3ms\n",
      "Speed: 0.0ms preprocess, 318.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.63, no fall 0.32, sitting 0.05, 8.5ms\n",
      "Speed: 15.5ms preprocess, 8.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.67, no fall 0.28, sitting 0.05, 0.0ms\n",
      "Speed: 2.2ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.70, no fall 0.25, sitting 0.05, 0.0ms\n",
      "Speed: 15.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.70, no fall 0.25, sitting 0.05, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.72, no fall 0.25, sitting 0.03, 22.4ms\n",
      "Speed: 0.0ms preprocess, 22.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 327.4ms\n",
      "Speed: 0.0ms preprocess, 327.4ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.60, no fall 0.35, sitting 0.05, 13.0ms\n",
      "Speed: 0.0ms preprocess, 13.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.53, no fall 0.41, sitting 0.07, 15.7ms\n",
      "Speed: 0.0ms preprocess, 15.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.56, no fall 0.39, sitting 0.06, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.56, no fall 0.38, sitting 0.06, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.60, no fall 0.34, sitting 0.06, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 329.9ms\n",
      "Speed: 0.0ms preprocess, 329.9ms inference, 15.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.62, no fall 0.33, sitting 0.05, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.54, no fall 0.40, sitting 0.06, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.60, no fall 0.34, sitting 0.06, 16.1ms\n",
      "Speed: 0.0ms preprocess, 16.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.52, no fall 0.42, sitting 0.06, 15.7ms\n",
      "Speed: 0.0ms preprocess, 15.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.62, no fall 0.33, sitting 0.06, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 283.5ms\n",
      "Speed: 0.0ms preprocess, 283.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.73, no fall 0.23, sitting 0.05, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.67, no fall 0.29, sitting 0.04, 15.7ms\n",
      "Speed: 0.0ms preprocess, 15.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.63, no fall 0.32, sitting 0.05, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.74, no fall 0.21, sitting 0.05, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.77, no fall 0.20, sitting 0.04, 9.5ms\n",
      "Speed: 6.5ms preprocess, 9.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 315.9ms\n",
      "Speed: 0.0ms preprocess, 315.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.51, no fall 0.44, sitting 0.06, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.51, no fall 0.42, sitting 0.06, 12.5ms\n",
      "Speed: 0.0ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.49, no fall 0.45, sitting 0.06, 15.5ms\n",
      "Speed: 0.0ms preprocess, 15.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.48, fall 0.46, sitting 0.06, 16.1ms\n",
      "Speed: 0.0ms preprocess, 16.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.49, no fall 0.46, sitting 0.05, 16.8ms\n",
      "Speed: 0.0ms preprocess, 16.8ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 330.1ms\n",
      "Speed: 0.0ms preprocess, 330.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.56, no fall 0.39, sitting 0.05, 16.0ms\n",
      "Speed: 0.0ms preprocess, 16.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.62, no fall 0.32, sitting 0.05, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.60, no fall 0.35, sitting 0.05, 7.3ms\n",
      "Speed: 0.0ms preprocess, 7.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.57, no fall 0.37, sitting 0.06, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.56, no fall 0.38, sitting 0.06, 15.7ms\n",
      "Speed: 0.0ms preprocess, 15.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 329.2ms\n",
      "Speed: 0.0ms preprocess, 329.2ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.55, no fall 0.38, sitting 0.06, 17.0ms\n",
      "Speed: 0.0ms preprocess, 17.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.62, no fall 0.33, sitting 0.05, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.60, no fall 0.36, sitting 0.05, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.57, no fall 0.38, sitting 0.04, 11.4ms\n",
      "Speed: 0.0ms preprocess, 11.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.51, no fall 0.43, sitting 0.05, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 312.9ms\n",
      "Speed: 0.0ms preprocess, 312.9ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.58, no fall 0.37, sitting 0.05, 15.9ms\n",
      "Speed: 0.0ms preprocess, 15.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.48, no fall 0.45, sitting 0.06, 15.9ms\n",
      "Speed: 0.0ms preprocess, 15.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.53, no fall 0.42, sitting 0.05, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.54, fall 0.40, sitting 0.06, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.48, no fall 0.46, sitting 0.06, 15.0ms\n",
      "Speed: 0.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 331.5ms\n",
      "Speed: 0.0ms preprocess, 331.5ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 no fall 0.55, fall 0.38, sitting 0.07, 16.7ms\n",
      "Speed: 0.0ms preprocess, 16.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.52, fall 0.40, sitting 0.08, 8.8ms\n",
      "Speed: 0.0ms preprocess, 8.8ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.47, fall 0.46, sitting 0.07, 7.0ms\n",
      "Speed: 0.9ms preprocess, 7.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.51, no fall 0.43, sitting 0.06, 6.3ms\n",
      "Speed: 1.1ms preprocess, 6.3ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.51, no fall 0.42, sitting 0.06, 13.9ms\n",
      "Speed: 0.0ms preprocess, 13.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 332.0ms\n",
      "Speed: 8.5ms preprocess, 332.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.72, no fall 0.24, sitting 0.05, 16.1ms\n",
      "Speed: 0.0ms preprocess, 16.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.61, no fall 0.35, sitting 0.05, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.53, no fall 0.42, sitting 0.05, 15.8ms\n",
      "Speed: 0.0ms preprocess, 15.8ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.59, no fall 0.36, sitting 0.05, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.59, no fall 0.37, sitting 0.04, 5.0ms\n",
      "Speed: 10.9ms preprocess, 5.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 315.7ms\n",
      "Speed: 0.0ms preprocess, 315.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 no fall 0.50, fall 0.41, sitting 0.09, 15.9ms\n",
      "Speed: 0.0ms preprocess, 15.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.53, fall 0.39, sitting 0.08, 17.7ms\n",
      "Speed: 0.0ms preprocess, 17.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.55, fall 0.38, sitting 0.07, 16.1ms\n",
      "Speed: 0.0ms preprocess, 16.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.56, fall 0.36, sitting 0.08, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.51, fall 0.42, sitting 0.07, 14.7ms\n",
      "Speed: 0.0ms preprocess, 14.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 298.7ms\n",
      "Speed: 0.0ms preprocess, 298.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.47, no fall 0.47, sitting 0.06, 5.7ms\n",
      "Speed: 12.0ms preprocess, 5.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.55, no fall 0.40, sitting 0.05, 6.5ms\n",
      "Speed: 0.0ms preprocess, 6.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.49, no fall 0.46, sitting 0.06, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.52, fall 0.42, sitting 0.06, 11.0ms\n",
      "Speed: 0.0ms preprocess, 11.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.53, no fall 0.42, sitting 0.05, 0.0ms\n",
      "Speed: 15.8ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 332.8ms\n",
      "Speed: 0.0ms preprocess, 332.8ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.54, no fall 0.40, sitting 0.06, 15.0ms\n",
      "Speed: 0.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.47, fall 0.46, sitting 0.07, 5.4ms\n",
      "Speed: 0.0ms preprocess, 5.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.48, no fall 0.45, sitting 0.06, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.47, fall 0.45, sitting 0.07, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.49, fall 0.43, sitting 0.07, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 315.3ms\n",
      "Speed: 0.0ms preprocess, 315.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 fall 0.57, no fall 0.37, sitting 0.06, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.54, no fall 0.40, sitting 0.07, 10.0ms\n",
      "Speed: 0.0ms preprocess, 10.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.57, no fall 0.37, sitting 0.06, 15.7ms\n",
      "Speed: 0.0ms preprocess, 15.7ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.55, no fall 0.38, sitting 0.07, 10.5ms\n",
      "Speed: 0.0ms preprocess, 10.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.50, no fall 0.42, sitting 0.08, 9.1ms\n",
      "Speed: 0.0ms preprocess, 9.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 345.3ms\n",
      "Speed: 0.0ms preprocess, 345.3ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 no fall 0.66, fall 0.25, sitting 0.09, 15.6ms\n",
      "Speed: 0.0ms preprocess, 15.6ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.62, fall 0.30, sitting 0.09, 12.0ms\n",
      "Speed: 0.0ms preprocess, 12.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.61, fall 0.31, sitting 0.09, 12.5ms\n",
      "Speed: 0.0ms preprocess, 12.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.53, fall 0.39, sitting 0.08, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.47, fall 0.44, sitting 0.08, 18.9ms\n",
      "Speed: 0.0ms preprocess, 18.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 317.1ms\n",
      "Speed: 0.0ms preprocess, 317.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 no fall 0.57, fall 0.34, sitting 0.09, 14.0ms\n",
      "Speed: 0.0ms preprocess, 14.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.49, no fall 0.43, sitting 0.07, 12.0ms\n",
      "Speed: 1.3ms preprocess, 12.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.55, no fall 0.37, sitting 0.08, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.49, fall 0.42, sitting 0.09, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.66, no fall 0.29, sitting 0.06, 0.0ms\n",
      "Speed: 15.6ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 362.7ms\n",
      "Speed: 5.5ms preprocess, 362.7ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 no fall 0.55, fall 0.37, sitting 0.08, 14.5ms\n",
      "Speed: 0.0ms preprocess, 14.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.57, fall 0.35, sitting 0.09, 16.4ms\n",
      "Speed: 0.0ms preprocess, 16.4ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.61, fall 0.32, sitting 0.08, 11.5ms\n",
      "Speed: 0.8ms preprocess, 11.5ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 no fall 0.48, fall 0.45, sitting 0.07, 10.1ms\n",
      "Speed: 5.5ms preprocess, 10.1ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 224x224 fall 0.52, no fall 0.41, sitting 0.07, 6.9ms\n",
      "Speed: 0.7ms preprocess, 6.9ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "\n",
      "0: 384x640 3 persons, 403.1ms\n",
      "Speed: 7.9ms preprocess, 403.1ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 224x224 no fall 0.49, fall 0.42, sitting 0.09, 21.0ms\n",
      "Speed: 0.0ms preprocess, 21.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"data/people_falling_3.mp4\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "scale_percent = 50 # percent of original size\n",
    "width = int(frame_width * scale_percent / 100)\n",
    "height = int(frame_height * scale_percent / 100)\n",
    "\n",
    "\n",
    "cap_writer = cv2.VideoWriter('data/result_detection.avi', cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                        25, (width, height))\n",
    "\n",
    "success, frame = cap.read()\n",
    "frame_num=0\n",
    "total_fps=0\n",
    "\n",
    "frame_num=0\n",
    "\n",
    "OBJECTS={}\n",
    "\n",
    "while success:\n",
    "    \n",
    "    \n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "  \n",
    "    # resize image\n",
    "    frame = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    \n",
    "    alarm = False\n",
    "    \n",
    "       \n",
    "    if frame_num%5==0:\n",
    "        #renew predictions every 5 frame\n",
    "        detections = detector(frame)[0]\n",
    "        detections = sv.Detections.from_ultralytics(detections)  \n",
    "                \n",
    "    detections = byte_tracker.update_with_detections(detections)\n",
    "        \n",
    "       \n",
    "    bboxes = detections.xyxy\n",
    "    confidences = detections.confidence\n",
    "    class_ids = detections.class_id\n",
    "    tracker_ids = detections.tracker_id\n",
    "    \n",
    "            \n",
    "    for pred_box, label, tracker_id in zip(bboxes, class_ids, tracker_ids):\n",
    "        \n",
    "        if label == 0:\n",
    "            bbox = [int(pred_box[0]), int(pred_box[1]), int(pred_box[2]), int(pred_box[3])]\n",
    "            width = bbox[2]-bbox[0]\n",
    "            height = bbox[3]-bbox[1]\n",
    "            ROI = frame[bbox[1]:bbox[1]+height, \n",
    "                    bbox[0]:bbox[0]+width]\n",
    "        \n",
    "            results = classifier(ROI)\n",
    "            second_class = classes[np.argmax(results[0].probs.data)]\n",
    "\n",
    "            if tracker_id not in OBJECTS:\n",
    "                OBJECTS[tracker_id] = {\n",
    "                    \"class_id\": label,\n",
    "                    \"last_bbox\": np.array(bbox),\n",
    "                    \"bbox\": np.array(bbox),\n",
    "                    \"second_class\": second_class,\n",
    "                    \"fall\": 0,\n",
    "                    \"bbox_offsets\": [0, 0],\n",
    "                    \"not_moving\": 0\n",
    "                    \n",
    "                }\n",
    "            else:\n",
    "                object = OBJECTS[tracker_id] \n",
    "                object['class_id']=label\n",
    "                object['last_bbox'] = OBJECTS[tracker_id]['bbox']\n",
    "                object['bbox']=np.array(bbox)\n",
    "                object['second_class']=second_class\n",
    "                object['bbox_offsets'] = compute_offsets(object['last_bbox'], object['bbox'])\n",
    "                if object['second_class']==\"fall\": #if fall\n",
    "                    object['fall']+=1\n",
    "                    if object['bbox_offsets']==[0, 0]:\n",
    "                        object['not_moving']+=1\n",
    "                    else:\n",
    "                        object['not_moving']=0    \n",
    "                else:\n",
    "                    object['fall']=0    \n",
    "                    object['not_moving']=0\n",
    "        \n",
    "            \n",
    "            cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), color=(0,0,255), thickness=2)\n",
    "            cv2.putText(frame, \"id: \"+str(tracker_id), (bbox[0], bbox[1]-10), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX , fontScale = 0.5,\n",
    "                        color=(0, 0, 0), thickness=1)\n",
    "        \n",
    "        #check falling objects\n",
    "        for ind, object in OBJECTS.items():\n",
    "            if object['fall']>=MAX_FALL:\n",
    "                 cv2.putText(frame, \"ALARM!\", (20, 50), \n",
    "                             cv2.FONT_HERSHEY_SIMPLEX , fontScale = 1, \n",
    "                             color=(0, 0, 255), thickness=5) \n",
    "                 if object['not_moving']>=MAX_NOT_MOVING:\n",
    "                     cv2.putText(frame, \"Not moving!\", (150, 50), \n",
    "                             cv2.FONT_HERSHEY_SIMPLEX , fontScale = 1, \n",
    "                             color=(0, 0, 255), thickness=3)\n",
    "        \n",
    "             \n",
    "                \n",
    "                    \n",
    "    #resize frame for videowriter\n",
    "    #frame = cv2.resize(frame, dsize=(640, 640), interpolation=cv2.INTER_CUBIC) \n",
    "    cap_writer.write(frame)      \n",
    "    cv2.imshow(\"frame\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    \n",
    "    success, frame = cap.read()\n",
    "    frame_num+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10025: {'class_id': 0,\n",
       "  'last_bbox': array([437, 261, 540, 451]),\n",
       "  'bbox': array([438, 261, 540, 451]),\n",
       "  'second_class': 'standing',\n",
       "  'fall': 0,\n",
       "  'bbox_offsets': [1, 0],\n",
       "  'not_moving': 0}}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(231, {'class_id': 0, 'bbox': array([203, 101, 384, 435]), 'second_class': 'standing', 'fall': 0})\n"
     ]
    }
   ],
   "source": [
    "for object in OBJECTS.items():\n",
    "    print(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={\"1\":1, \"2\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training YOLO cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = \"datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-cls.pt to 'yolov8s-cls.pt'...\n",
      "100%|██████████| 12.2M/12.2M [00:00<00:00, 14.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolov8s-cls.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.0.211 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.0.209  Python-3.8.9 torch-2.1.0+cpu CPU (11th Gen Intel Core(TM) i5-1145G7 2.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8s-cls.pt, data=datasets, epochs=50, patience=50, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train2\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Projects\\PeopleFallDetection\\datasets\\train... found 11870 images in 3 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m None...\n",
      "\u001b[34m\u001b[1mtest:\u001b[0m C:\\Projects\\PeopleFallDetection\\datasets\\test... found 1490 images in 3 classes  \n",
      "Overriding model.yaml nc=1000 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    661763  ultralytics.nn.modules.head.Classify         [512, 3]                      \n",
      "YOLOv8s-cls summary: 99 layers, 5084579 parameters, 5084579 gradients, 12.6 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Projects\\PeopleFallDetection\\datasets\\train... 11870 images, 0 corrupt: 100%|██████████| 11870/11870 [00:04<00:00, 2480.56it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Projects\\PeopleFallDetection\\datasets\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Projects\\PeopleFallDetection\\datasets\\test... 1490 images, 0 corrupt: 100%|██████████| 1490/1490 [00:00<00:00, 4624.66it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Projects\\PeopleFallDetection\\datasets\\test.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       1/50         0G    0.07011         14        224: 100%|██████████| 742/742 [18:21<00:00,  1.48s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 47/47 [01:08<00:00,  1.46s/it]\n",
      "                   all      0.838          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       2/50         0G    0.02657         14        224: 100%|██████████| 742/742 [17:20<00:00,  1.40s/it]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 47/47 [00:34<00:00,  1.36it/s]\n",
      "                   all      0.857          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       3/50         0G    0.02868         14        224: 100%|██████████| 742/742 [12:03<00:00,  1.02it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 47/47 [00:30<00:00,  1.54it/s]\n",
      "                   all      0.835          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       4/50         0G      0.018         14        224: 100%|██████████| 742/742 [11:29<00:00,  1.08it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 47/47 [00:30<00:00,  1.53it/s]\n",
      "                   all      0.857          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       5/50         0G    0.01096         14        224: 100%|██████████| 742/742 [11:46<00:00,  1.05it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 47/47 [00:30<00:00,  1.53it/s]\n",
      "                   all      0.833          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       6/50         0G   0.007455         14        224: 100%|██████████| 742/742 [11:40<00:00,  1.06it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 47/47 [00:31<00:00,  1.48it/s]\n",
      "                   all       0.85          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       7/50         0G   0.004167         14        224: 100%|██████████| 742/742 [11:29<00:00,  1.08it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 47/47 [00:31<00:00,  1.49it/s]\n",
      "                   all      0.855          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n",
      "       8/50         0G   0.005447         16        224:  70%|███████   | 522/742 [08:53<03:44,  1.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\PeopleFallDetection\\test.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projects/PeopleFallDetection/test.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49mDATASET_FOLDER, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, imgsz\u001b[39m=\u001b[39;49m\u001b[39m224\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Projects\\PeopleFallDetection\\.venv\\lib\\site-packages\\ultralytics\\engine\\model.py:338\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[0;32m    337\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 338\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    339\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Projects\\PeopleFallDetection\\.venv\\lib\\site-packages\\ultralytics\\engine\\trainer.py:190\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    187\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n",
      "File \u001b[1;32mc:\\Projects\\PeopleFallDetection\\.venv\\lib\\site-packages\\ultralytics\\engine\\trainer.py:344\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39m*\u001b[39m i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items) \u001b[39m/\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \\\n\u001b[0;32m    341\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_items\n\u001b[0;32m    343\u001b[0m \u001b[39m# Backward\u001b[39;00m\n\u001b[1;32m--> 344\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mscale(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    346\u001b[0m \u001b[39m# Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\u001b[39;00m\n\u001b[0;32m    347\u001b[0m \u001b[39mif\u001b[39;00m ni \u001b[39m-\u001b[39m last_opt_step \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccumulate:\n",
      "File \u001b[1;32mc:\\Projects\\PeopleFallDetection\\.venv\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Projects\\PeopleFallDetection\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(data=DATASET_FOLDER, epochs=50, imgsz=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING  '--project' does not require leading dashes '--', updating to 'project'.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alena_Prakonina\\AppData\\Local\\Programs\\Python\\Python38\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\Alena_Prakonina\\AppData\\Local\\Programs\\Python\\Python38\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Projects\\PeopleFallDetection\\.venv\\Scripts\\yolo.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"c:\\projects\\peoplefalldetection\\.venv\\lib\\site-packages\\ultralytics\\cfg\\__init__.py\", line 378, in entrypoint\n",
      "    raise SyntaxError(f\"'{colorstr('red', 'bold', a)}' is a valid YOLO argument but is missing an '=' sign \"\n",
      "SyntaxError: '\u001b[31m\u001b[1mproject\u001b[0m' is a valid YOLO argument but is missing an '=' sign to set its value, i.e. try 'project=None'\n",
      "\n",
      "    Arguments received: ['yolo', 'classify', 'train', 'imgsz=224', 'epochs=50', 'data=datasets/train/', 'model=yolov8s-cls.pt', '--project', 'yolov8s-people_fall_3_classes', 'device=0']. Ultralytics 'yolo' commands use the following syntax:\n",
      "\n",
      "        yolo TASK MODE ARGS\n",
      "\n",
      "        Where   TASK (optional) is one of ('detect', 'segment', 'classify', 'pose')\n",
      "                MODE (required) is one of ('train', 'val', 'predict', 'export', 'track', 'benchmark')\n",
      "                ARGS (optional) are any number of custom 'arg=value' pairs like 'imgsz=320' that override defaults.\n",
      "                    See all ARGS at https://docs.ultralytics.com/usage/cfg or with 'yolo cfg'\n",
      "\n",
      "    1. Train a detection model for 10 epochs with an initial learning_rate of 0.01\n",
      "        yolo train data=coco128.yaml model=yolov8n.pt epochs=10 lr0=0.01\n",
      "\n",
      "    2. Predict a YouTube video using a pretrained segmentation model at image size 320:\n",
      "        yolo predict model=yolov8n-seg.pt source='https://youtu.be/LNwODJXcvt4' imgsz=320\n",
      "\n",
      "    3. Val a pretrained detection model at batch-size 1 and image size 640:\n",
      "        yolo val model=yolov8n.pt data=coco128.yaml batch=1 imgsz=640\n",
      "\n",
      "    4. Export a YOLOv8n classification model to ONNX format at image size 224 by 128 (no TASK required)\n",
      "        yolo export model=yolov8n-cls.pt format=onnx imgsz=224,128\n",
      "\n",
      "    5. Run special commands:\n",
      "        yolo help\n",
      "        yolo checks\n",
      "        yolo version\n",
      "        yolo settings\n",
      "        yolo copy-cfg\n",
      "        yolo cfg\n",
      "\n",
      "    Docs: https://docs.ultralytics.com\n",
      "    Community: https://community.ultralytics.com\n",
      "    GitHub: https://github.com/ultralytics/ultralytics\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!yolo classify train imgsz=224 epochs=50 data={DATASET_FOLDER} model=yolov8s-cls.pt --project yolov8s-people_fall_3_classes device=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_not_moving(frame, not_moving):\n",
    "    if not_moving>=10:\n",
    "        cv2.putText(frame, \"Not moving\",\n",
    "                    (150, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX , fontScale = 1, \n",
    "                             color=(0, 0, 255), thickness=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9987037037037036\n",
      "0.03327739197530864\n",
      "0.0238695987654321\n",
      "0.01863425925925926\n",
      "0.03042631172839506\n",
      "0.023227237654320988\n",
      "0.01912229938271605\n",
      "0.020482253086419752\n",
      "0.028512731481481483\n",
      "0.02947530864197531\n",
      "0.030495756172839505\n",
      "0.03898533950617284\n",
      "0.026786265432098767\n",
      "0.03362654320987654\n",
      "0.027662037037037037\n",
      "0.03857638888888889\n",
      "0.0510570987654321\n",
      "0.05089699074074074\n",
      "0.06965663580246914\n",
      "0.08076774691358024\n",
      "0.0691724537037037\n",
      "0.07252893518518519\n",
      "0.06911651234567902\n",
      "0.07480902777777777\n",
      "0.06703317901234568\n",
      "0.06967785493827161\n",
      "0.06793788580246914\n",
      "0.0655420524691358\n",
      "0.06500964506172839\n",
      "0.06569251543209877\n",
      "0.0619579475308642\n",
      "0.0575462962962963\n",
      "0.06144868827160494\n",
      "0.06543016975308642\n",
      "0.0654320987654321\n",
      "0.060271990740740744\n",
      "0.06544753086419754\n",
      "0.06255979938271605\n",
      "0.06515432098765432\n",
      "0.06457947530864197\n",
      "0.057629243827160496\n",
      "0.05886766975308642\n",
      "0.06327160493827161\n",
      "0.06466242283950617\n",
      "0.0619849537037037\n",
      "0.06371527777777777\n",
      "0.06276427469135802\n",
      "0.06542631172839507\n",
      "0.06360725308641975\n",
      "0.0676466049382716\n",
      "0.06304012345679012\n",
      "0.06220679012345679\n",
      "0.06332754629629629\n",
      "0.06332368827160494\n",
      "0.06266010802469135\n",
      "0.06612268518518519\n",
      "0.06044367283950617\n",
      "0.06873456790123457\n",
      "0.06427662037037037\n",
      "0.05334104938271605\n",
      "0.05620370370370371\n",
      "0.0641261574074074\n",
      "0.05462384259259259\n",
      "0.05581404320987654\n",
      "0.048406635802469136\n",
      "0.05780285493827161\n",
      "0.05738618827160494\n",
      "0.050374228395061726\n",
      "0.047928240740740743\n",
      "0.05446566358024691\n",
      "0.05167052469135802\n",
      "0.04366319444444444\n",
      "0.043395061728395065\n",
      "0.048684413580246916\n",
      "0.04546489197530864\n",
      "0.043701774691358025\n",
      "0.043092206790123454\n",
      "0.043431712962962964\n",
      "0.04522569444444444\n",
      "0.043015046296296294\n",
      "0.039357638888888886\n",
      "0.04556327160493827\n",
      "0.04958333333333333\n",
      "0.04670717592592592\n",
      "0.04660493827160494\n",
      "0.04149498456790123\n",
      "0.04534722222222222\n",
      "0.043146219135802466\n",
      "0.04457175925925926\n",
      "0.04333526234567901\n",
      "0.03323881172839506\n",
      "0.03911651234567901\n",
      "0.042312885802469134\n",
      "0.043578317901234565\n",
      "0.04410300925925926\n",
      "0.04355516975308642\n",
      "0.040484182098765434\n",
      "0.04572145061728395\n",
      "0.037328317901234566\n",
      "0.03986689814814815\n",
      "0.04322337962962963\n",
      "0.043327546296296295\n",
      "0.043902391975308645\n",
      "0.042986111111111114\n",
      "0.0436207561728395\n",
      "0.04029320987654321\n",
      "0.039878472222222225\n",
      "0.03215277777777778\n",
      "0.03878472222222222\n",
      "0.036222993827160495\n",
      "0.03222222222222222\n",
      "0.030125385802469137\n",
      "0.03594714506172839\n",
      "0.03316936728395062\n",
      "0.0355054012345679\n",
      "0.034212962962962966\n",
      "0.03183256172839506\n",
      "0.03364583333333333\n",
      "0.0338966049382716\n",
      "0.030362654320987656\n",
      "0.029012345679012345\n",
      "0.024857253086419753\n",
      "0.027374614197530865\n",
      "0.025731095679012345\n",
      "0.0256616512345679\n",
      "0.024089506172839507\n",
      "0.02089313271604938\n",
      "0.02281635802469136\n",
      "0.02117283950617284\n",
      "0.023638117283950617\n",
      "0.02476466049382716\n",
      "0.019425154320987653\n",
      "0.021739969135802468\n",
      "0.024517746913580246\n",
      "0.02812114197530864\n",
      "0.02470871913580247\n",
      "0.026898148148148147\n",
      "0.021182484567901234\n",
      "0.021371527777777777\n",
      "0.02146604938271605\n",
      "0.02427854938271605\n",
      "0.023601466049382715\n",
      "0.02529320987654321\n",
      "0.02566550925925926\n",
      "0.024801311728395063\n",
      "0.023389274691358024\n",
      "0.02474344135802469\n",
      "0.025841049382716048\n",
      "0.029799382716049382\n",
      "0.023470293209876543\n",
      "0.02230131172839506\n",
      "0.025457175925925925\n",
      "0.025266203703703704\n",
      "0.02090470679012346\n",
      "0.022349537037037036\n",
      "0.02574074074074074\n",
      "0.017957175925925925\n",
      "0.020300925925925927\n",
      "0.023665123456790123\n",
      "0.022195216049382717\n",
      "0.02523533950617284\n",
      "0.024243827160493826\n",
      "0.02931520061728395\n",
      "0.028152006172839507\n",
      "0.023329475308641975\n",
      "0.03300347222222222\n",
      "0.036653163580246916\n",
      "0.035152391975308644\n",
      "0.034861111111111114\n",
      "0.034382716049382714\n",
      "0.032081404320987654\n",
      "0.028406635802469136\n",
      "0.027880015432098764\n",
      "0.040324074074074075\n",
      "0.03778935185185185\n",
      "0.03875192901234568\n",
      "0.03402391975308642\n",
      "0.037723765432098766\n",
      "0.026761188271604938\n",
      "0.023798225308641976\n",
      "0.02226273148148148\n",
      "0.02822723765432099\n",
      "0.02687885802469136\n",
      "0.025642361111111112\n",
      "0.02538773148148148\n",
      "0.0262866512345679\n",
      "0.02777006172839506\n",
      "0.028589891975308642\n",
      "0.02842206790123457\n",
      "0.02254050925925926\n",
      "0.023989197530864197\n",
      "0.024506172839506173\n",
      "0.026383101851851852\n",
      "0.02342013888888889\n",
      "0.02355324074074074\n",
      "0.02050347222222222\n",
      "0.025875771604938272\n",
      "0.02238425925925926\n",
      "0.020054012345679013\n",
      "0.019135802469135803\n",
      "0.02257908950617284\n",
      "0.021790123456790125\n",
      "0.023526234567901233\n",
      "0.021325231481481483\n",
      "0.022233796296296297\n",
      "0.020032793209876543\n",
      "0.0190991512345679\n",
      "0.015869984567901233\n",
      "0.017339891975308643\n",
      "0.020231481481481482\n",
      "0.016786265432098765\n",
      "0.01662615740740741\n",
      "0.015968364197530865\n",
      "0.01544945987654321\n",
      "0.013005401234567901\n",
      "0.01697337962962963\n",
      "0.012893518518518518\n",
      "0.014849537037037038\n",
      "0.013701774691358025\n",
      "0.015698302469135804\n",
      "0.013825231481481482\n",
      "0.01396412037037037\n",
      "0.017555941358024692\n",
      "0.023188657407407408\n",
      "0.02554591049382716\n",
      "0.021315586419753087\n",
      "0.022544367283950616\n",
      "0.016840277777777777\n",
      "0.01607638888888889\n",
      "0.01589699074074074\n",
      "0.016591435185185185\n",
      "0.022548225308641975\n",
      "0.018171296296296297\n",
      "0.023304398148148147\n",
      "0.0211304012345679\n",
      "0.01751929012345679\n",
      "0.020341435185185185\n",
      "0.02254050925925926\n",
      "0.018628472222222223\n",
      "0.025277777777777777\n",
      "0.015914351851851853\n",
      "0.01900270061728395\n",
      "0.02803240740740741\n",
      "0.024662422839506173\n",
      "0.025690586419753088\n",
      "0.027064043209876543\n",
      "0.024592978395061728\n",
      "0.026257716049382717\n",
      "0.027843364197530866\n",
      "0.029072145061728394\n",
      "0.03356481481481482\n",
      "0.03574074074074074\n",
      "0.0350733024691358\n",
      "0.03833140432098765\n",
      "0.03578896604938272\n",
      "0.03194637345679013\n",
      "0.02217013888888889\n",
      "0.03596064814814815\n",
      "0.03528935185185185\n",
      "0.04644675925925926\n",
      "0.03979166666666667\n",
      "0.04417631172839506\n",
      "0.036120756172839504\n",
      "0.030868055555555555\n",
      "0.031942515432098764\n",
      "0.03395833333333333\n",
      "0.045133101851851855\n",
      "0.0379783950617284\n",
      "0.040796682098765434\n",
      "0.03648533950617284\n",
      "0.03371720679012346\n",
      "0.03560570987654321\n",
      "0.035688657407407405\n",
      "0.037218364197530864\n",
      "0.03961226851851852\n",
      "0.03716628086419753\n",
      "0.035787037037037034\n",
      "0.035709876543209874\n",
      "0.03201388888888889\n",
      "0.03396990740740741\n",
      "0.034535108024691355\n",
      "0.03698881172839506\n",
      "0.03679783950617284\n",
      "0.03580246913580247\n",
      "0.03524691358024691\n",
      "0.03411844135802469\n",
      "0.03108024691358025\n",
      "0.03279513888888889\n",
      "0.025875771604938272\n",
      "0.028742283950617283\n",
      "0.018636188271604938\n",
      "0.023422067901234567\n",
      "0.034834104938271604\n",
      "0.02847608024691358\n",
      "0.024577546296296295\n",
      "0.024035493827160494\n",
      "0.032172067901234565\n",
      "0.029513888888888888\n",
      "0.025108024691358026\n",
      "0.02377121913580247\n",
      "0.021184413580246912\n",
      "0.023742283950617282\n",
      "0.02757908950617284\n",
      "0.026327160493827162\n",
      "0.02007716049382716\n",
      "0.021261574074074075\n",
      "0.02031635802469136\n",
      "0.017816358024691357\n",
      "0.02032986111111111\n",
      "0.02144483024691358\n",
      "0.022488425925925926\n",
      "0.019135802469135803\n",
      "0.024753086419753087\n",
      "0.023655478395061727\n",
      "0.019909336419753086\n",
      "0.025044367283950618\n",
      "0.023657407407407408\n",
      "0.023308256172839506\n",
      "0.02580054012345679\n",
      "0.020881558641975308\n",
      "0.02421489197530864\n",
      "0.029284336419753088\n",
      "0.028443287037037038\n",
      "0.026005015432098766\n",
      "0.03132523148148148\n",
      "0.02707175925925926\n",
      "0.02515432098765432\n",
      "0.02072337962962963\n",
      "0.025675154320987655\n",
      "0.025291280864197532\n",
      "0.02665895061728395\n",
      "0.022970679012345677\n",
      "0.02254050925925926\n",
      "0.021423611111111112\n",
      "0.024702932098765434\n",
      "0.023825231481481482\n",
      "0.027004243827160494\n",
      "0.02787615740740741\n",
      "0.02777391975308642\n",
      "0.027320601851851853\n",
      "0.02029513888888889\n",
      "0.023134645061728396\n",
      "0.021076388888888888\n",
      "0.02193287037037037\n",
      "0.026005015432098766\n",
      "0.019498456790123457\n",
      "0.017092978395061728\n",
      "0.026770833333333334\n",
      "0.019226466049382718\n",
      "0.019845679012345678\n",
      "0.01698688271604938\n",
      "0.0213695987654321\n",
      "0.01827932098765432\n",
      "0.01642361111111111\n",
      "0.02046682098765432\n",
      "0.01913966049382716\n",
      "0.018231095679012346\n",
      "0.01693479938271605\n",
      "0.021902006172839505\n",
      "0.02155671296296296\n",
      "0.023686342592592592\n",
      "0.018578317901234567\n",
      "0.020104166666666666\n",
      "0.027085262345679012\n",
      "0.019554398148148147\n",
      "0.02657986111111111\n",
      "0.020804398148148148\n",
      "0.022272376543209876\n",
      "0.025652006172839505\n",
      "0.026703317901234567\n",
      "0.031118827160493828\n",
      "0.027393904320987653\n",
      "0.027328317901234567\n",
      "0.026670524691358024\n",
      "0.02877121913580247\n",
      "0.02945216049382716\n",
      "0.03234182098765432\n",
      "0.03857445987654321\n",
      "0.0363329475308642\n",
      "0.03954668209876543\n",
      "0.03277199074074074\n",
      "0.03095871913580247\n",
      "0.03319058641975309\n",
      "0.03288194444444444\n",
      "0.0386400462962963\n",
      "0.03961226851851852\n",
      "0.03938850308641975\n",
      "0.03544174382716049\n",
      "0.03552276234567901\n",
      "0.03344521604938271\n",
      "0.03733217592592593\n",
      "0.03654320987654321\n",
      "0.03564043209876543\n",
      "0.03449652777777778\n",
      "0.035397376543209874\n",
      "0.03652006172839506\n",
      "0.03720100308641975\n",
      "0.034380787037037036\n",
      "0.038823302469135804\n",
      "0.03591820987654321\n",
      "0.03583912037037037\n",
      "0.035931712962962964\n",
      "0.038703703703703705\n",
      "0.03119405864197531\n",
      "0.03208333333333333\n",
      "0.03296875\n",
      "0.0323070987654321\n",
      "0.02834490740740741\n",
      "0.028252314814814813\n",
      "0.029074074074074075\n",
      "0.03236882716049383\n",
      "0.03113425925925926\n",
      "0.029496527777777778\n",
      "0.03489776234567901\n",
      "0.03710262345679012\n",
      "0.04427469135802469\n",
      "0.04108603395061729\n",
      "0.041603009259259256\n",
      "0.04242283950617284\n",
      "0.041332947530864195\n",
      "0.03786844135802469\n",
      "0.03531057098765432\n",
      "0.04426311728395062\n",
      "0.040817901234567903\n",
      "0.03554398148148148\n",
      "0.03737461419753087\n",
      "0.03869020061728395\n",
      "0.03600501543209877\n",
      "0.032658179012345676\n",
      "0.03544753086419753\n",
      "0.03347993827160494\n",
      "0.03386188271604938\n",
      "0.03532407407407408\n",
      "0.03462962962962963\n",
      "0.033505015432098766\n",
      "0.029743441358024692\n",
      "0.031435185185185184\n",
      "0.028368055555555556\n",
      "0.03415316358024691\n",
      "0.03441550925925926\n",
      "0.034498456790123457\n",
      "0.030987654320987653\n",
      "0.030966435185185184\n",
      "0.02514081790123457\n",
      "0.029444444444444443\n",
      "0.027168209876543208\n",
      "0.028155864197530866\n",
      "0.03450810185185185\n",
      "0.03058641975308642\n",
      "0.024347993827160495\n",
      "0.02620949074074074\n",
      "0.025324074074074075\n",
      "0.021930941358024692\n",
      "0.024699074074074075\n",
      "0.024429012345679013\n",
      "0.024926697530864197\n",
      "0.026550925925925926\n",
      "0.025989583333333333\n",
      "0.026730324074074073\n",
      "0.027901234567901233\n",
      "0.024322916666666666\n",
      "0.023800154320987654\n",
      "0.02318287037037037\n",
      "0.02396604938271605\n",
      "0.02422067901234568\n",
      "0.024346064814814813\n",
      "0.021554783950617284\n",
      "0.026770833333333334\n",
      "0.027029320987654322\n",
      "0.02165895061728395\n",
      "0.019932484567901233\n",
      "0.02519483024691358\n",
      "0.023680555555555555\n",
      "0.021413966049382716\n",
      "0.026547067901234567\n",
      "0.02742091049382716\n",
      "0.025300925925925925\n",
      "0.0264891975308642\n",
      "0.02748263888888889\n",
      "0.03111111111111111\n",
      "0.02663773148148148\n",
      "0.029185956790123455\n",
      "0.026114969135802468\n",
      "0.02388695987654321\n",
      "0.030364583333333334\n",
      "0.03130401234567901\n",
      "0.03276427469135802\n",
      "0.03791087962962963\n",
      "0.03653935185185185\n",
      "0.03877314814814815\n",
      "0.034249614197530864\n",
      "0.035285493827160494\n",
      "0.03799189814814815\n",
      "0.041178626543209876\n",
      "0.03906828703703704\n",
      "0.03743827160493827\n",
      "0.04376736111111111\n",
      "0.04761188271604938\n",
      "0.039697145061728396\n",
      "0.03708912037037037\n",
      "0.041311728395061725\n",
      "0.04509645061728395\n",
      "0.05010802469135803\n",
      "0.0484471450617284\n",
      "0.04395640432098766\n",
      "0.04428433641975309\n",
      "0.0463599537037037\n",
      "0.04392554012345679\n",
      "0.03806327160493827\n",
      "0.044376929012345676\n",
      "0.03907793209876543\n",
      "0.036703317901234565\n",
      "0.037926311728395064\n",
      "0.04609182098765432\n",
      "0.038682484567901236\n",
      "0.03770061728395062\n",
      "0.0423070987654321\n",
      "0.03948688271604938\n",
      "0.04169560185185185\n",
      "0.04384259259259259\n",
      "0.03446180555555556\n",
      "0.03899305555555556\n",
      "0.04224344135802469\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\PeopleFallDetection\\test.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/PeopleFallDetection/test.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m success, frame_prev \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/PeopleFallDetection/test.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m fgMask_prev \u001b[39m=\u001b[39m backSub\u001b[39m.\u001b[39mapply(frame_prev)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Projects/PeopleFallDetection/test.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m frame_prev \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(frame_prev, dsize\u001b[39m=\u001b[39;49m(width, height))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/PeopleFallDetection/test.ipynb#X25sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m fgMask_prev \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mresize(fgMask_prev, dsize\u001b[39m=\u001b[39m(width, height))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Projects/PeopleFallDetection/test.ipynb#X25sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m check_not_moving(frame_prev, not_moving) \n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4062: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(\"data/me.mp4\")\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "scale_percent = 50 # percent of original size\n",
    "width = int(frame_width * scale_percent / 100)\n",
    "height = int(frame_height * scale_percent / 100)\n",
    "    \n",
    "backSub = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "not_moving=0\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    \n",
    "    success, frame_prev = cap.read()\n",
    "      \n",
    "    \n",
    "    fgMask_prev = backSub.apply(frame_prev)\n",
    "    \n",
    "    frame_prev = cv2.resize(frame_prev, dsize=(width, height))\n",
    "    fgMask_prev = cv2.resize(fgMask_prev, dsize=(width, height))\n",
    "    \n",
    "    check_not_moving(frame_prev, not_moving) \n",
    "    \n",
    "    cv2.imshow('Frame', frame_prev)\n",
    "    cv2.imshow('FG Mask', fgMask_prev)\n",
    "    \n",
    "    \n",
    "    success, frame_next = cap.read()\n",
    "    \n",
    "    fgMask_next = backSub.apply(frame_next)\n",
    "    \n",
    "    frame_next = cv2.resize(frame_next, dsize=(width, height))\n",
    "    fgMask_next = cv2.resize(fgMask_next, dsize=(width, height))\n",
    "    \n",
    "    diff = cv2.absdiff(fgMask_prev, fgMask_next)   \n",
    "    thresh_diff = cv2.threshold(diff, 15, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    \n",
    "    cv2.imshow(\"difference\", diff)\n",
    "    \n",
    "    # Calculate the difference between the 2 images\n",
    "    total_pixels = frame_prev.shape[0] * frame_prev.shape[1] * 1.0\n",
    "    diff_on_pixels = cv2.countNonZero(thresh_diff) * 1.0\n",
    "    difference_measure = diff_on_pixels / total_pixels\n",
    "    \n",
    "    print(difference_measure)\n",
    "    \n",
    "    if difference_measure<=0.03:\n",
    "        not_moving+=1   \n",
    "    else:\n",
    "        not_moving = 0    \n",
    "          \n",
    "    check_not_moving(frame_next, not_moving) \n",
    "    \n",
    "       \n",
    "    cv2.imshow('Frame', frame_next)\n",
    "    cv2.imshow('FG Mask', fgMask_next)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "    \n",
    "    \n",
    "    success, frame = cap.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
